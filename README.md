# Post processing script for cleaning metadata generated from xml2workbench.py to Prepare the cleaned Metadata for Migrating data to Louisiana Digital Library new website
The script was created to prepare a CSV sheet for running the Islandora Workbench to import relational data from the old Louisiana Digital Library to the new website. It starts with the initial CSV metadata made created from XML files from the website. It has two stages:
### 1) Creating needed columns and dropping the unwanted fields from Metadata.
- a) In the first step, the script gets the metadata CSV, uses the existence of OBJ files in the directory, and processes them to generate the correct pattern for the file column according to the existence of those OBJ data for each node.
- b) Next, it will create needed columns such as a file, field_weight, and field_member_of and add those file directory patterns to the file columns for each node(webpage).
- c) Also, it drops unwanted columns that are not in Drupal default. The existence of these columns will stop the workbench process, as these fields do not exist in Drupal.

### 2) Processing the RDF files to write down the relationships(parent and children pages)
- 1) With looping into the file directory and RDF files in the folder, the script uses tags, attributes, and texts in RDF and cleans their name in an order that drupal can understand them.
- 2) Using three main tags inside the RDFs, which determine the relationship between nodes, the parent_id column can be filled out. Having information about relationships is crucial in the Workbench data ingestion stage.  
- 3) Also, by looping through  RDFs we get the correct information about field_weight, which indicates the order of children's pages.
- 4) Also, by looping through  RDFs, we get the correct information about field_weight, which indicates the order of children's pages.
